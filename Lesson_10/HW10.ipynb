{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5156,
     "status": "ok",
     "timestamp": 1619789089261,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "CNvjhDyAKk3U",
    "outputId": "3166c525-fc56-4fd1-92b2-2f1ac4ac18a6"
   },
   "outputs": [],
   "source": [
    "# !wget http://www.manythings.org/anki/rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1813,
     "status": "ok",
     "timestamp": 1619789098992,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "83bg17Lr-7XK",
    "outputId": "91ec0134-b351-4d0a-c8ac-215affefed97"
   },
   "outputs": [],
   "source": [
    "# !mkdir rus-eng\n",
    "# !unzip rus-eng.zip -d rus-eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1619789101916,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "7o5L92efMMhf",
    "outputId": "6ac53232-05f5-449b-8cfc-e0dda693d57f"
   },
   "outputs": [],
   "source": [
    "# !ls /content/rus-eng/ -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_file = \"./rus-eng/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1619789112313,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "yV9lZXQXNbnH",
    "outputId": "2f509965-ecec-4988-cffa-09ab9f954f13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"<start> i can't go . <end>\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENG, RUS]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12954,
     "status": "ok",
     "timestamp": 1619808083270,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "cTbSbBz55QtF",
    "outputId": "5a5b0b4e-da16-4483-8e56-5e07fd79f0bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go . <end>\n",
      "<start> марш ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[0])\n",
    "print(ru[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1619789980135,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "C8j9g9AnIeZV",
    "outputId": "510da3ec-5a6d-49b9-a0da-29c693f1aec5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(464010, 464010)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1619790003654,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "4QILQkOs3jFG",
    "outputId": "ddc672e5-2231-4985-b550-2dbddd4468b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1619790005935,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "VXukARTDd7MT",
    "outputId": "a3606064-f48d-4e81-94d5-9c30d5e90ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6 ----> том\n",
      "37 ----> нам\n",
      "7 ----> не\n",
      "536 ----> поможет\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> tom\n",
      "79 ----> won't\n",
      "51 ----> help\n",
      "48 ----> us\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 500\n",
    "units = 2024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1619790037553,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "5f42bae2-eefc-492f-f5c5-ad5215d549cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(TensorShape([256, 15]), TensorShape([256, 11]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27335,
     "status": "ok",
     "timestamp": 1619790133885,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "60gSVh05Jl6l",
    "outputId": "4eaa568c-966c-4a9b-895d-4d5e22d20e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (256, 2024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1619791577723,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "XKcypC0AGeLR",
    "outputId": "6f51f5ef-6d92-4933-d202-e2d512cbba37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([256, 7350])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1619791547930,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "6y0HF-zMF_vp",
    "outputId": "69a0893f-61c6-40c5-bce6-5911c3c4355b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([256, 2024])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10943081,
     "status": "ok",
     "timestamp": 1619803077637,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "ddefjBMa3jF0",
    "outputId": "b54245db-e06a-46b0-edc0-fab6afaba17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6597\n",
      "Epoch 1 Batch 100 Loss 1.7707\n",
      "Epoch 1 Batch 200 Loss 1.3191\n",
      "Epoch 1 Batch 300 Loss 1.1996\n",
      "Epoch 1 Loss 1.6281\n",
      "Time taken for 1 epoch 135.44741344451904 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0384\n",
      "Epoch 2 Batch 100 Loss 0.9758\n",
      "Epoch 2 Batch 200 Loss 0.9201\n",
      "Epoch 2 Batch 300 Loss 0.7747\n",
      "Epoch 2 Loss 0.9154\n",
      "Time taken for 1 epoch 140.96252584457397 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6562\n",
      "Epoch 3 Batch 100 Loss 0.5420\n",
      "Epoch 3 Batch 200 Loss 0.5214\n",
      "Epoch 3 Batch 300 Loss 0.4736\n",
      "Epoch 3 Loss 0.5675\n",
      "Time taken for 1 epoch 122.42408299446106 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3327\n",
      "Epoch 4 Batch 100 Loss 0.3609\n",
      "Epoch 4 Batch 200 Loss 0.3507\n",
      "Epoch 4 Batch 300 Loss 0.3382\n",
      "Epoch 4 Loss 0.3449\n",
      "Time taken for 1 epoch 148.88564085960388 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2085\n",
      "Epoch 5 Batch 100 Loss 0.2043\n",
      "Epoch 5 Batch 200 Loss 0.2345\n",
      "Epoch 5 Batch 300 Loss 0.2368\n",
      "Epoch 5 Loss 0.2200\n",
      "Time taken for 1 epoch 122.54100155830383 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1190\n",
      "Epoch 6 Batch 100 Loss 0.1528\n",
      "Epoch 6 Batch 200 Loss 0.1407\n",
      "Epoch 6 Batch 300 Loss 0.1936\n",
      "Epoch 6 Loss 0.1519\n",
      "Time taken for 1 epoch 150.38076853752136 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1084\n",
      "Epoch 7 Batch 100 Loss 0.1116\n",
      "Epoch 7 Batch 200 Loss 0.1269\n",
      "Epoch 7 Batch 300 Loss 0.1360\n",
      "Epoch 7 Loss 0.1156\n",
      "Time taken for 1 epoch 122.81585597991943 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0768\n",
      "Epoch 8 Batch 100 Loss 0.1020\n",
      "Epoch 8 Batch 200 Loss 0.0897\n",
      "Epoch 8 Batch 300 Loss 0.1157\n",
      "Epoch 8 Loss 0.0957\n",
      "Time taken for 1 epoch 151.81793665885925 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0661\n",
      "Epoch 9 Batch 100 Loss 0.0695\n",
      "Epoch 9 Batch 200 Loss 0.1019\n",
      "Epoch 9 Batch 300 Loss 0.0902\n",
      "Epoch 9 Loss 0.0843\n",
      "Time taken for 1 epoch 128.35533905029297 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0652\n",
      "Epoch 10 Batch 100 Loss 0.0664\n",
      "Epoch 10 Batch 200 Loss 0.0709\n",
      "Epoch 10 Batch 300 Loss 0.0717\n",
      "Epoch 10 Loss 0.0796\n",
      "Time taken for 1 epoch 162.64372277259827 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0670\n",
      "Epoch 11 Batch 100 Loss 0.0619\n",
      "Epoch 11 Batch 200 Loss 0.0761\n",
      "Epoch 11 Batch 300 Loss 0.1010\n",
      "Epoch 11 Loss 0.0758\n",
      "Time taken for 1 epoch 133.04691648483276 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0466\n",
      "Epoch 12 Batch 100 Loss 0.0745\n",
      "Epoch 12 Batch 200 Loss 0.0961\n",
      "Epoch 12 Batch 300 Loss 0.0787\n",
      "Epoch 12 Loss 0.0741\n",
      "Time taken for 1 epoch 162.49962878227234 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0751\n",
      "Epoch 13 Batch 100 Loss 0.0592\n",
      "Epoch 13 Batch 200 Loss 0.0701\n",
      "Epoch 13 Batch 300 Loss 0.0931\n",
      "Epoch 13 Loss 0.0729\n",
      "Time taken for 1 epoch 137.8358883857727 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0560\n",
      "Epoch 14 Batch 100 Loss 0.0555\n",
      "Epoch 14 Batch 200 Loss 0.0654\n",
      "Epoch 14 Batch 300 Loss 0.0852\n",
      "Epoch 14 Loss 0.0721\n",
      "Time taken for 1 epoch 154.34695029258728 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0450\n",
      "Epoch 15 Batch 100 Loss 0.0678\n",
      "Epoch 15 Batch 200 Loss 0.0692\n",
      "Epoch 15 Batch 300 Loss 0.0901\n",
      "Epoch 15 Loss 0.0707\n",
      "Time taken for 1 epoch 124.27893090248108 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0378\n",
      "Epoch 16 Batch 100 Loss 0.0638\n",
      "Epoch 16 Batch 200 Loss 0.0780\n",
      "Epoch 16 Batch 300 Loss 0.1043\n",
      "Epoch 16 Loss 0.0712\n",
      "Time taken for 1 epoch 159.16117024421692 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0507\n",
      "Epoch 17 Batch 100 Loss 0.0506\n",
      "Epoch 17 Batch 200 Loss 0.0756\n",
      "Epoch 17 Batch 300 Loss 0.0722\n",
      "Epoch 17 Loss 0.0688\n",
      "Time taken for 1 epoch 126.81020760536194 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0755\n",
      "Epoch 18 Batch 100 Loss 0.0637\n",
      "Epoch 18 Batch 200 Loss 0.0835\n",
      "Epoch 18 Batch 300 Loss 0.0692\n",
      "Epoch 18 Loss 0.0672\n",
      "Time taken for 1 epoch 159.4879972934723 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0454\n",
      "Epoch 19 Batch 100 Loss 0.0599\n",
      "Epoch 19 Batch 200 Loss 0.0854\n",
      "Epoch 19 Batch 300 Loss 0.0766\n",
      "Epoch 19 Loss 0.0662\n",
      "Time taken for 1 epoch 127.06464076042175 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0517\n",
      "Epoch 20 Batch 100 Loss 0.0713\n",
      "Epoch 20 Batch 200 Loss 0.0719\n",
      "Epoch 20 Batch 300 Loss 0.0772\n",
      "Epoch 20 Loss 0.0632\n",
      "Time taken for 1 epoch 163.68352603912354 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0541\n",
      "Epoch 21 Batch 100 Loss 0.0590\n",
      "Epoch 21 Batch 200 Loss 0.0726\n",
      "Epoch 21 Batch 300 Loss 0.0767\n",
      "Epoch 21 Loss 0.0622\n",
      "Time taken for 1 epoch 127.14013075828552 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0479\n",
      "Epoch 22 Batch 100 Loss 0.0534\n",
      "Epoch 22 Batch 200 Loss 0.0885\n",
      "Epoch 22 Batch 300 Loss 0.0716\n",
      "Epoch 22 Loss 0.0606\n",
      "Time taken for 1 epoch 153.16777110099792 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0565\n",
      "Epoch 23 Batch 100 Loss 0.0425\n",
      "Epoch 23 Batch 200 Loss 0.0806\n",
      "Epoch 23 Batch 300 Loss 0.0677\n",
      "Epoch 23 Loss 0.0594\n",
      "Time taken for 1 epoch 127.41883659362793 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0581\n",
      "Epoch 24 Batch 100 Loss 0.0577\n",
      "Epoch 24 Batch 200 Loss 0.0550\n",
      "Epoch 24 Batch 300 Loss 0.0844\n",
      "Epoch 24 Loss 0.0582\n",
      "Time taken for 1 epoch 158.74504852294922 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0404\n",
      "Epoch 25 Batch 100 Loss 0.0541\n",
      "Epoch 25 Batch 200 Loss 0.0422\n",
      "Epoch 25 Batch 300 Loss 0.0579\n",
      "Epoch 25 Loss 0.0573\n",
      "Time taken for 1 epoch 127.1813473701477 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0458\n",
      "Epoch 26 Batch 100 Loss 0.0536\n",
      "Epoch 26 Batch 200 Loss 0.0596\n",
      "Epoch 26 Batch 300 Loss 0.0649\n",
      "Epoch 26 Loss 0.0557\n",
      "Time taken for 1 epoch 153.60112500190735 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0423\n",
      "Epoch 27 Batch 100 Loss 0.0633\n",
      "Epoch 27 Batch 200 Loss 0.0554\n",
      "Epoch 27 Batch 300 Loss 0.0729\n",
      "Epoch 27 Loss 0.0548\n",
      "Time taken for 1 epoch 127.21830654144287 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0478\n",
      "Epoch 28 Batch 100 Loss 0.0615\n",
      "Epoch 28 Batch 200 Loss 0.0577\n",
      "Epoch 28 Batch 300 Loss 0.0704\n",
      "Epoch 28 Loss 0.0543\n",
      "Time taken for 1 epoch 154.59672498703003 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0363\n",
      "Epoch 29 Batch 100 Loss 0.0482\n",
      "Epoch 29 Batch 200 Loss 0.0706\n",
      "Epoch 29 Batch 300 Loss 0.0607\n",
      "Epoch 29 Loss 0.0543\n",
      "Time taken for 1 epoch 127.21100354194641 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0425\n",
      "Epoch 30 Batch 100 Loss 0.0570\n",
      "Epoch 30 Batch 200 Loss 0.0704\n",
      "Epoch 30 Batch 300 Loss 0.0557\n",
      "Epoch 30 Loss 0.0527\n",
      "Time taken for 1 epoch 152.7347903251648 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0292\n",
      "Epoch 31 Batch 100 Loss 0.0569\n",
      "Epoch 31 Batch 200 Loss 0.0511\n",
      "Epoch 31 Batch 300 Loss 0.0486\n",
      "Epoch 31 Loss 0.0525\n",
      "Time taken for 1 epoch 127.18954825401306 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0524\n",
      "Epoch 32 Batch 100 Loss 0.0449\n",
      "Epoch 32 Batch 200 Loss 0.0670\n",
      "Epoch 32 Batch 300 Loss 0.0702\n",
      "Epoch 32 Loss 0.0513\n",
      "Time taken for 1 epoch 150.06093001365662 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0449\n",
      "Epoch 33 Batch 100 Loss 0.0434\n",
      "Epoch 33 Batch 200 Loss 0.0496\n",
      "Epoch 33 Batch 300 Loss 0.0638\n",
      "Epoch 33 Loss 0.0507\n",
      "Time taken for 1 epoch 127.27703309059143 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0396\n",
      "Epoch 34 Batch 100 Loss 0.0474\n",
      "Epoch 34 Batch 200 Loss 0.0643\n",
      "Epoch 34 Batch 300 Loss 0.0656\n",
      "Epoch 34 Loss 0.0500\n",
      "Time taken for 1 epoch 150.03656840324402 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0332\n",
      "Epoch 35 Batch 100 Loss 0.0383\n",
      "Epoch 35 Batch 200 Loss 0.0393\n",
      "Epoch 35 Batch 300 Loss 0.0627\n",
      "Epoch 35 Loss 0.0483\n",
      "Time taken for 1 epoch 127.30517554283142 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0399\n",
      "Epoch 36 Batch 100 Loss 0.0444\n",
      "Epoch 36 Batch 200 Loss 0.0498\n",
      "Epoch 36 Batch 300 Loss 0.0598\n",
      "Epoch 36 Loss 0.0468\n",
      "Time taken for 1 epoch 147.9700915813446 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0362\n",
      "Epoch 37 Batch 100 Loss 0.0433\n",
      "Epoch 37 Batch 200 Loss 0.0482\n",
      "Epoch 37 Batch 300 Loss 0.0523\n",
      "Epoch 37 Loss 0.0456\n",
      "Time taken for 1 epoch 127.32030892372131 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0384\n",
      "Epoch 38 Batch 100 Loss 0.0621\n",
      "Epoch 38 Batch 200 Loss 0.0455\n",
      "Epoch 38 Batch 300 Loss 0.0578\n",
      "Epoch 38 Loss 0.0452\n",
      "Time taken for 1 epoch 148.06001472473145 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0453\n",
      "Epoch 39 Batch 100 Loss 0.0312\n",
      "Epoch 39 Batch 200 Loss 0.0397\n",
      "Epoch 39 Batch 300 Loss 0.0488\n",
      "Epoch 39 Loss 0.0446\n",
      "Time taken for 1 epoch 127.19202446937561 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0421\n",
      "Epoch 40 Batch 100 Loss 0.0341\n",
      "Epoch 40 Batch 200 Loss 0.0582\n",
      "Epoch 40 Batch 300 Loss 0.0589\n",
      "Epoch 40 Loss 0.0442\n",
      "Time taken for 1 epoch 148.4750690460205 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0320\n",
      "Epoch 41 Batch 100 Loss 0.0370\n",
      "Epoch 41 Batch 200 Loss 0.0463\n",
      "Epoch 41 Batch 300 Loss 0.0615\n",
      "Epoch 41 Loss 0.0446\n",
      "Time taken for 1 epoch 127.41717720031738 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0342\n",
      "Epoch 42 Batch 100 Loss 0.0336\n",
      "Epoch 42 Batch 200 Loss 0.0502\n",
      "Epoch 42 Batch 300 Loss 0.0439\n",
      "Epoch 42 Loss 0.0447\n",
      "Time taken for 1 epoch 146.81296825408936 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0273\n",
      "Epoch 43 Batch 100 Loss 0.0430\n",
      "Epoch 43 Batch 200 Loss 0.0660\n",
      "Epoch 43 Batch 300 Loss 0.0466\n",
      "Epoch 43 Loss 0.0444\n",
      "Time taken for 1 epoch 127.18137431144714 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0342\n",
      "Epoch 44 Batch 100 Loss 0.0370\n",
      "Epoch 44 Batch 200 Loss 0.0409\n",
      "Epoch 44 Batch 300 Loss 0.0603\n",
      "Epoch 44 Loss 0.0444\n",
      "Time taken for 1 epoch 148.06962180137634 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0334\n",
      "Epoch 45 Batch 100 Loss 0.0442\n",
      "Epoch 45 Batch 200 Loss 0.0500\n",
      "Epoch 45 Batch 300 Loss 0.0453\n",
      "Epoch 45 Loss 0.0429\n",
      "Time taken for 1 epoch 127.15468835830688 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0317\n",
      "Epoch 46 Batch 100 Loss 0.0260\n",
      "Epoch 46 Batch 200 Loss 0.0350\n",
      "Epoch 46 Batch 300 Loss 0.0436\n",
      "Epoch 46 Loss 0.0410\n",
      "Time taken for 1 epoch 147.5840504169464 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0328\n",
      "Epoch 47 Batch 100 Loss 0.0309\n",
      "Epoch 47 Batch 200 Loss 0.0451\n",
      "Epoch 47 Batch 300 Loss 0.0467\n",
      "Epoch 47 Loss 0.0400\n",
      "Time taken for 1 epoch 127.27236270904541 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0350\n",
      "Epoch 48 Batch 100 Loss 0.0402\n",
      "Epoch 48 Batch 200 Loss 0.0393\n",
      "Epoch 48 Batch 300 Loss 0.0471\n",
      "Epoch 48 Loss 0.0398\n",
      "Time taken for 1 epoch 147.26965641975403 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0370\n",
      "Epoch 49 Batch 100 Loss 0.0305\n",
      "Epoch 49 Batch 200 Loss 0.0412\n",
      "Epoch 49 Batch 300 Loss 0.0429\n",
      "Epoch 49 Loss 0.0390\n",
      "Time taken for 1 epoch 127.36942648887634 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0266\n",
      "Epoch 50 Batch 100 Loss 0.0282\n",
      "Epoch 50 Batch 200 Loss 0.0520\n",
      "Epoch 50 Batch 300 Loss 0.0641\n",
      "Epoch 50 Loss 0.0393\n",
      "Time taken for 1 epoch 147.54375100135803 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1608145599781,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "a5bf709a-7e66-4fd8-aca9-777497144965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1416f43cd60>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1619808753710,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "WrAM0FDomq3E",
    "outputId": "d366d6cc-cc03-4e35-a65a-9d06dcbfff36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> здесь хорошо . <end>\n",
      "Predicted translation: it's good to here . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Здесь хорошо.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1619808761495,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "5bhFfwcIMX5i",
    "outputId": "f89a2d14-a72b-4477-9d76-36837baaefff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я не смогу поехать . <end>\n",
      "Predicted translation: i can't go . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Я не смогу поехать.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1619808768959,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "zSx2iM36EZQZ",
    "outputId": "42bc96b4-37c0-439f-fff9-224fdc58c527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> вы еще дома ? <end>\n",
      "Predicted translation: are you still home ? <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Вы еще дома?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3LLCx3ZE0Ls",
    "outputId": "b64aa087-8232-474e-e3c7-98c186081845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> вы все еще дома ? <end>\n",
      "Predicted translation: are you still home ? <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Вы все еще дома?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1619808777901,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "DUQVLVqUE1YW",
    "outputId": "6d768ecc-e145-4a4a-b313-0986c44bc1cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> попробуй сделать это . <end>\n",
      "Predicted translation: try to do that . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Попробуй сделать это.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1619808783771,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "f09_hUFx9EJh",
    "outputId": "73980799-f0f9-4ff5-835c-ced55d9a4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я люблю , когда идет снег . <end>\n",
      "Predicted translation: i like it in snow . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Я люблю, когда идет снег.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я люблю , когда падает снег . <end>\n",
      "Predicted translation: i like french books . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Я люблю, когда падает снег.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7c5p8rmkHQG",
    "outputId": "d4682d71-f778-41f5-e4a9-2e1235976123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я никогда такого не делаю . <end>\n",
      "Predicted translation: i never do that . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Я никогда такого не делаю.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "jdXES85KkTVS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> я хочу поесть мясо . <end>\n",
      "Predicted translation: i want to eat meat . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Я хочу поесть мясо.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
