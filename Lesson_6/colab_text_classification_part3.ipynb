{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_text_classification_part3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sUtoed20cRJJ"},"source":["# Загрузка текста"]},{"cell_type":"code","metadata":{"id":"baYFZMW_bJHh"},"source":["import tensorflow as tf\n","\n","import tensorflow_datasets as tfds\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YWVWjyIkffau"},"source":["Тексты трех переводов выполнили:\n","\n"," - [Вильям Купер](https://en.wikipedia.org/wiki/William_Cowper) — [текст](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n","\n"," - [Эрл Эдвард](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [текст](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n","\n","- [Сэмюель Батлер](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [текст](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n","\n","Текстовые файлы использованные в этом руководстве были подвергнуты некоторым типичным задачам предварительной обработки, в основном удаление материала - шапка и футер документа, нумерация строк, заголовки глав. Скачайте эти файлы локально."]},{"cell_type":"code","metadata":{"id":"4YlKQthEYlFw","colab":{"base_uri":"https://localhost:8080/","height":137},"executionInfo":{"status":"ok","timestamp":1607187166777,"user_tz":-180,"elapsed":6411,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"635029c2-4b81-4daa-aa20-ec8d7e7a1bb5"},"source":["DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n","FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n","\n","for name in FILE_NAMES:\n","  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n","  \n","parent_dir = os.path.dirname(text_dir)\n","\n","parent_dir"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n","819200/815980 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n","811008/809730 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n","811008/807992 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/root/.keras/datasets'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"q3sDy6nuXoNp"},"source":["## Загрузите текст в датасеты\n","\n","Переберите файлы, загружая каждый в свой датасет.\n","\n","Каждый пример нужно пометить индивидуально, так что используйте `tf.data.Dataset.map` чтобы применить функцию расставляющую метки каждому элементу. Она переберет каждую запись в датасете возвращая пару (`example, label`)."]},{"cell_type":"code","metadata":{"id":"K0BjCOpOh7Ch"},"source":["def labeler(example, index):\n","  return example, tf.cast(index, tf.int64)  \n","\n","labeled_data_sets = []\n","\n","for i, file_name in enumerate(FILE_NAMES):\n","  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n","  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n","  labeled_data_sets.append(labeled_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8PHK5J_cXE5"},"source":["Объедините эти размеченные наборы данных в один и перемешайте его.\n"]},{"cell_type":"code","metadata":{"id":"6jAeYkTIi9-2"},"source":["BUFFER_SIZE = 50000\n","BATCH_SIZE = 64\n","TAKE_SIZE = 5000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qd544E-Sh63L"},"source":["all_labeled_data = labeled_data_sets[0]\n","for labeled_dataset in labeled_data_sets[1:]:\n","  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n","  \n","all_labeled_data = all_labeled_data.shuffle(\n","    BUFFER_SIZE, reshuffle_each_iteration=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4JEHrJXeG5k"},"source":["Вы можете использовать `tf.data.Dataset.take` и `print`, чтобы посмотреть как выглядят пары `(example, label)`. Свойство `numpy` показывает каждое значение тензора."]},{"cell_type":"code","metadata":{"id":"gywKlN0xh6u5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187206451,"user_tz":-180,"elapsed":2006,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"173ffbc4-7b41-4c57-c391-d0e8b4d092ec"},"source":["for ex in all_labeled_data.take(5):\n","  print(ex)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: shape=(), dtype=string, numpy=b'More than a woman or a puny child:'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'The Trojans, Hector and \\xc3\\x86neas rush'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'\"Hector, thou son of Priam, leave me not'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'Some say hereafter, Menelaus bore'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b\"To avenge her slaughter'd brothers on his head.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5rrpU2_sfDh0"},"source":["## Закодируйте текстовые строки числами\n","\n","Модели машинного обучения работают с числами, не словами, так что строковые значения необходимо конвертировать в списки с числами. Чтобы сделать это поставьте в соответствие каждому слову свое число.\n","\n","### Создайте словарь\n","\n","Сперва создайте словарь токенизировав текст в коллекцию отдельных отличающихся слов. Есть несколько способов сделать это и в TensorFlow и в Python. В этом учебнике:\n","\n","1. Переберите `numpy` значения всех примеров.\n","2. Используйте `tfds.features.text.Tokenizer` чтобы разбить их на токены.\n","3. Соберите эти токены в множество Python чтобы избавиться от дубликатов\n","4. Получите размер словаря для последующего использования."]},{"cell_type":"code","metadata":{"id":"6wRWyRx5Zhcx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YkHtbGnDh6mg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187227980,"user_tz":-180,"elapsed":6868,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"f3153ba2-4963-42cc-82cc-46f6650ea8fc"},"source":["tokenizer = tfds.deprecated.text.Tokenizer()\n","# tf.keras.preprocessing.text.Tokenizer()\n","tf.keras.preprocessing.sequence\n","vocabulary_set = set()\n","for text_tensor, _ in all_labeled_data:\n","  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n","  vocabulary_set.update(some_tokens)\n","\n","vocab_size = len(vocabulary_set)\n","vocab_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["17178"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"gkxJIVAth6j0"},"source":["encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6S5Qyabi-vo"},"source":["Вы можете посмотреть одну строку чтобы увидеть как выглядит результат работы кодировщика."]},{"cell_type":"code","metadata":{"id":"jgxPZaxUuTbk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187238290,"user_tz":-180,"elapsed":1625,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"0e57dc0d-416a-429b-f57f-647159679a78"},"source":["example_text = next(iter(all_labeled_data))[0].numpy()\n","print(example_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["b'More than a woman or a puny child:'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XoVpKR3qj5yb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187238829,"user_tz":-180,"elapsed":566,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"7ac0eb2a-e70b-4a13-abef-6fa3dd13da43"},"source":["encoded_example = encoder.encode(example_text)\n","print(encoded_example)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[4385, 13505, 5499, 3662, 3117, 5499, 5549, 14418]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p9qHM0v8k_Mg"},"source":["Теперь запустите кодировщик на датасете обернув его в `tf.py_function` и передав в метод `map` датасета."]},{"cell_type":"code","metadata":{"id":"HcIQ7LOTh6eT"},"source":["def encode(text_tensor, label):\n","  encoded_text = encoder.encode(text_tensor.numpy())\n","  return encoded_text, label\n","\n","def encode_map_fn(text, label):\n","  # py_func doesn't set the shape of the returned tensors.\n","  encoded_text, label = tf.py_function(encode, \n","                                       inp=[text, label], \n","                                       Tout=(tf.int64, tf.int64))\n","\n","  # `tf.data.Datasets` work best if all components have a shape set\n","  #  so set the shapes manually: \n","  encoded_text.set_shape([None])\n","  label.set_shape([])\n","\n","  return encoded_text, label\n","\n","\n","all_encoded_data = all_labeled_data.map(encode_map_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_YZToSXSm0qr"},"source":["## Разбейте датасет на тестовую и обучающую выборки\n","\n","Используйте `tf.data.Dataset.take` и `tf.data.Dataset.skip` чтобы создать небольшой тестовый и большой обучающий датасеты.\n","\n","Перед передачей в модель датасет должны быть разбиты на пакеты. Обычно количество записей в пакете и их размерность должно быть одинаковые."]},{"cell_type":"code","metadata":{"id":"r-rmbijQh6bf"},"source":["train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n","train_data = train_data.padded_batch(BATCH_SIZE)\n","\n","test_data = all_encoded_data.take(TAKE_SIZE)\n","test_data = test_data.padded_batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xdz7SVwmqi1l"},"source":["Сейчас, `test_data` и `train_data` являются не коллекциями пар (`example, label`), а коллекциями пакетов. Каждый пакет это пара вида (*много примеров*, *много меток*) представленная в виде массивов.\n","\n","Чтобы проиллюстрировать:"]},{"cell_type":"code","metadata":{"id":"kMslWfuwoqpB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187247230,"user_tz":-180,"elapsed":2004,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"fbb0f577-802d-4413-e078-b735dfbc2aba"},"source":["sample_text, sample_labels = next(iter(test_data))\n","\n","sample_text[0], sample_labels[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(17,), dtype=int64, numpy=\n"," array([ 4385, 13505,  5499,  3662,  3117,  5499,  5549, 14418,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])>,\n"," <tf.Tensor: shape=(), dtype=int64, numpy=1>)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"UI4I6_Sa0vWu"},"source":["Так как мы ввели новую кодировку токенов (нуль использовался для заполнения), размер словаря увеличился на единицу."]},{"cell_type":"code","metadata":{"id":"IlD1Lli91vuc"},"source":["vocab_size += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8SUhGFNsmRi"},"source":["## Постройте модель\n"]},{"cell_type":"code","metadata":{"id":"QJgI1pow2YR9"},"source":["model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DR6-ctbY638P"},"source":["model.add(tf.keras.layers.Embedding(vocab_size, 64))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6rnq6DN_WUs"},"source":["model.add(tf.keras.layers.GlobalAveragePooling1D())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTEaNSnLCsv5"},"source":["# Один или более плотных слоев.\n","# Отредактируйте список в строке `for` чтобы поэкспериментировать с размером слоев.\n","for units in [64, 64]:\n","  model.add(tf.keras.layers.Dense(units, activation='relu'))\n","\n","# Выходной слой. Первый аргумент - число меток.\n","model.add(tf.keras.layers.Dense(3, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLHPU8q5DLi_"},"source":["Наконец скомпилируйте модель. Для softmax категоризационной модели используйте `sparse_categorical_crossentropy` в виде функции потерь. Вы можете попробовать другие оптимизаторы, но `adam` очень часто используемая."]},{"cell_type":"code","metadata":{"id":"KqqLYK71rrxi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187335380,"user_tz":-180,"elapsed":1379,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"9ea3e4e6-0f45-4e27-919c-cfa97b64f021"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 64)          1099456   \n","_________________________________________________________________\n","global_average_pooling1d (Gl (None, 64)                0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                4160      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 1,107,971\n","Trainable params: 1,107,971\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pkTBUVO4h6Y5"},"source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DM-HLo5NDhql"},"source":["## Обучите модель\n"]},{"cell_type":"code","metadata":{"id":"aLtO33tNh6V8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607187430631,"user_tz":-180,"elapsed":76782,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"3e768fbc-dcab-47aa-8e6d-14236c473fab"},"source":["model.fit(train_data, epochs=3, validation_data=test_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","697/697 [==============================] - 11s 16ms/step - loss: 0.5495 - accuracy: 0.7392 - val_loss: 0.4135 - val_accuracy: 0.8146\n","Epoch 2/3\n","697/697 [==============================] - 11s 16ms/step - loss: 0.3182 - accuracy: 0.8615 - val_loss: 0.3790 - val_accuracy: 0.8302\n","Epoch 3/3\n","697/697 [==============================] - 12s 17ms/step - loss: 0.2423 - accuracy: 0.8952 - val_loss: 0.3919 - val_accuracy: 0.8330\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f175e0497f0>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"KTPCYf_Jh6TH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec59cb79-8c6a-4e74-8dc3-061e92c462f7"},"source":["eval_loss, eval_acc = model.evaluate(test_data)\n","\n","print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["79/79 [==============================] - 2s 23ms/step - loss: 0.3701 - accuracy: 0.8384\n","\n","Eval loss: 0.370, Eval accuracy: 0.838\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OYKrznEDsfW8"},"source":[""],"execution_count":null,"outputs":[]}]}